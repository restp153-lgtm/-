ggplot(team_summary_svm, aes(x = actual_win_rate, y = avg_pred_prob, label = TEAM_ABBREVIATION)) +
geom_point(color = "darkgreen", size = 3) +
geom_text(vjust = -0.5, size = 3) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(
title = "SVM 模型（機率版）：球隊預測勝率 vs 真實勝率",
x = "真實勝率",
y = "預測勝率"
) +
theme_minimal()
GameLogs2$svm_error <- abs(GameLogs2$svm_pred_prob - GameLogs2$WL_num)
# --- Step 5. 計算每隊平均預測勝率 ---
team_summary_svm <- GameLogs2 %>%
group_by(TEAM_ABBREVIATION) %>%
summarise(
avg_pred_prob = mean(svm_pred_prob, na.rm = TRUE),
actual_win_rate = mean(WL_num, na.rm = TRUE),
.groups = "drop"
)
# --- Step 6. 畫散點圖 ---
ggplot(team_summary_svm, aes(x = actual_win_rate, y = avg_pred_prob, label = TEAM_ABBREVIATION)) +
geom_point(color = "darkgreen", size = 3) +
geom_text(vjust = -0.5, size = 3) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(
title = "SVM 模型（機率版）：球隊預測勝率 vs 真實勝率",
x = "真實勝率",
y = "預測勝率"
) +
theme_minimal()
# --- Step 3. 用整個資料建立最終模型 ---
svm_final <- svm(WL_num ~ ., data = svm_data, kernel = "radial", scale = TRUE, probability = F)
# --- Step 4. 預測逐場勝負 ---
svm_pred_prob <- predict(svm_final, svm_data, probability = TRUE)
GameLogs2$svm_pred_prob <- attr(svm_pred_prob, "probabilities")[, "1"]
GameLogs2$svm_error <- abs(GameLogs2$svm_pred_prob - GameLogs2$WL_num)
# --- Step 4. 預測逐場勝負 ---
svm_pred_prob <- predict(svm_final, svm_data, probability = F)
GameLogs2$svm_pred_prob <- attr(svm_pred_prob, "probabilities")[, "1"]
GameLogs2$svm_error <- abs(GameLogs2$svm_pred_prob - GameLogs2$WL_num)
# --- Step 3. 用整個資料建立最終模型 ---
svm_final <- svm(WL_num ~ ., data = svm_data, kernel = "radial", scale = TRUE, probability = F)
# --- Step 4. 預測逐場勝負 ---
svm_pred_prob <- predict(svm_final, svm_data, probability = F)
GameLogs2$svm_pred_prob <- attr(svm_pred_prob, "probabilities")[, "1"]
GameLogs2$svm_error <- abs(GameLogs2$svm_pred_prob - GameLogs2$WL_num)
library(dplyr)
library(e1071) # 提供 SVM 函數
library(ggplot2)
# --- Step 1. 準備資料 (不變) ---
num_vars <- names(GameLogs2)[sapply(GameLogs2, is.numeric)]
num_vars <- setdiff(num_vars, c("WL_num", "GameNo", "error"))
svm_data <- GameLogs2 %>%
select(all_of(c("WL_num", num_vars)))
svm_data$WL_num <- as.factor(svm_data$WL_num) # SVM 分類
# --- Step 2. K 折交叉驗證 (使用預設參數) ---
set.seed(123)
K <- 5
n <- nrow(svm_data)
folds <- sample(rep(1:K, length.out = n))
accuracy_list_default <- c()
for(k in 1:K){
train_idx <- which(folds != k)
test_idx  <- which(folds == k)
train_data <- svm_data[train_idx, ]
test_data  <- svm_data[test_idx, ]
# 建立 SVM 模型（RBF kernel, 預設 C=1, gamma=1/n_features）
svm_model_default <- svm(WL_num ~ ., data = train_data, kernel = "radial", scale = TRUE)
# 預測
pred <- predict(svm_model_default, test_data)
# 計算準確率
acc <- mean(pred == test_data$WL_num)
accuracy_list_default <- c(accuracy_list_default, acc)
}
cat("【基礎 SVM】5 折交叉驗證平均準確率:", round(mean(accuracy_list_default),4), "\n")
# --- Step 3. 超參數調優（Grid Search）以尋找最佳模型 ---
cat("\n--- 開始 SVM 超參數調優 (Grid Search) ---\n")
set.seed(456)
tune_parameters <- list(
# 懲罰係數 C: 權衡錯誤分類的懲罰與模型的複雜度
cost = 10^(-1:3), # 0.1, 1, 10, 100, 1000
# RBF 核參數 gamma: 決定單個訓練樣本的影響範圍
gamma = 10^(-3:1) # 0.001, 0.01, 0.1, 1, 10
)
# 進行交叉驗證的網格搜索 (預設使用 10-fold CV)
svm_tuned <- tune.svm(
WL_num ~ .,
data = svm_data,
kernel = "radial",
scale = TRUE,
ranges = tune_parameters,
# 調整交叉驗證折數 (如果數據量大，可以減少折數加速)
tunecontrol = tune.control(sampling = "cross", cross = 5)
)
library(dplyr)
library(e1071) # 提供 SVM 函數
library(ggplot2)
# --- Step 1. 準備資料 (不變) ---
num_vars <- names(GameLogs2)[sapply(GameLogs2, is.numeric)]
num_vars <- setdiff(num_vars, c("WL_num", "GameNo", "error"))
svm_data <- GameLogs2 %>%
select(all_of(c("WL_num", num_vars)))
svm_data$WL_num <- as.factor(svm_data$WL_num) # SVM 分類
# --- Step 2. K 折交叉驗證 (使用預設參數) ---
set.seed(123)
K <- 5
n <- nrow(svm_data)
folds <- sample(rep(1:K, length.out = n))
accuracy_list_default <- c()
for(k in 1:K){
train_idx <- which(folds != k)
test_idx  <- which(folds == k)
train_data <- svm_data[train_idx, ]
test_data  <- svm_data[test_idx, ]
# 建立 SVM 模型（RBF kernel, 預設 C=1, gamma=1/n_features）
svm_model_default <- svm(WL_num ~ ., data = train_data, kernel = "radial", scale = TRUE)
# 預測
pred <- predict(svm_model_default, test_data)
# 計算準確率
acc <- mean(pred == test_data$WL_num)
accuracy_list_default <- c(accuracy_list_default, acc)
}
cat("【基礎 SVM】5 折交叉驗證平均準確率:", round(mean(accuracy_list_default),4), "\n")
# --- Step 3. 超參數調優（Grid Search）以尋找最佳模型 ---
cat("\n--- 開始 SVM 超參數調優 (Grid Search) ---\n")
set.seed(456)
# A. 定義要搜索的參數範圍
tune_parameters <- list(
cost = 10^(-1:3), # 0.1, 1, 10, 100, 1000
gamma = 10^(-3:1) # 0.001, 0.01, 0.1, 1, 10
)
# B. 定義交叉驗證控制參數
# 指定使用 5 折交叉驗證 (cross = 5)
my_tune_control <- tune.control(sampling = "cross", cross = 5)
# C. 運行網格搜索
svm_tuned <- tune.svm(
WL_num ~ .,
data = svm_data,
kernel = "radial",
scale = TRUE,
ranges = tune_parameters,      # 傳遞參數範圍
tunecontrol = my_tune_control # 傳遞交叉驗證控制
)
# --- Step 1. 準備資料 (不變) ---
num_vars <- names(GameLogs2)[sapply(GameLogs2, is.numeric)]
num_vars <- setdiff(num_vars, c("WL_num", "GameNo", "error"))
svm_data <- GameLogs2 %>%
select(all_of(c("WL_num", num_vars)))
svm_data$WL_num <- as.factor(svm_data$WL_num) # SVM 分類
# --- Step 2. K 折交叉驗證 (使用預設參數) ---
set.seed(123)
K <- 5
n <- nrow(svm_data)
folds <- sample(rep(1:K, length.out = n))
accuracy_list_default <- c()
for(k in 1:K){
train_idx <- which(folds != k)
test_idx  <- which(folds == k)
train_data <- svm_data[train_idx, ]
test_data  <- svm_data[test_idx, ]
# 建立 SVM 模型（RBF kernel, 預設 C=1, gamma=1/n_features）
svm_model_default <- svm(WL_num ~ ., data = train_data, kernel = "radial", scale = TRUE)
# 預測
pred <- predict(svm_model_default, test_data)
# 計算準確率
acc <- mean(pred == test_data$WL_num)
accuracy_list_default <- c(accuracy_list_default, acc)
}
cat("【基礎 SVM】5 折交叉驗證平均準確率:", round(mean(accuracy_list_default),4), "\n")
# --- Step 3. 超參數調優（Grid Search）以尋找最佳模型 ---
cat("\n--- 開始 SVM 超參數調優 (Grid Search) ---\n")
set.seed(456)
# A. 定義要搜索的參數範圍
tune_parameters <- list(
cost = 10^(-1:3), # 0.1, 1, 10, 100, 1000
gamma = 10^(-3:1) # 0.001, 0.01, 0.1, 1, 10
)
# B. 定義交叉驗證控制參數
# 指定使用 5 折交叉驗證 (cross = 5)
my_tune_control <- tune.control(sampling = "cross", cross = 5)
# C. 運行網格搜索
svm_tuned <- tune.svm(
WL_num ~ .,
data = svm_data,
kernel = "radial",
scale = TRUE,
ranges = tune_parameters,      # 傳遞參數範圍
tunecontrol = my_tune_control # 傳遞交叉驗證控制
)
best_C <- svm_tuned$best.parameters$cost
best_gamma <- svm_tuned$best.parameters$gamma
# --- Step 5. 預測逐場勝負 (使用優化後的模型) ---
svm_pred_prob <- predict(svm_final_optimized, svm_data, probability = TRUE)
# --- Step 1. 準備資料 (不變) ---
num_vars <- names(GameLogs2)[sapply(GameLogs2, is.numeric)]
num_vars <- setdiff(num_vars, c("WL_num", "GameNo", "error"))
svm_data <- GameLogs2 %>%
select(all_of(c("WL_num", num_vars)))
svm_data$WL_num <- as.factor(svm_data$WL_num) # SVM 分類
# --- Step 2. K 折交叉驗證 (使用預設參數) ---
set.seed(123)
K <- 5
n <- nrow(svm_data)
folds <- sample(rep(1:K, length.out = n))
accuracy_list_default <- c()
for(k in 1:K){
train_idx <- which(folds != k)
test_idx  <- which(folds == k)
train_data <- svm_data[train_idx, ]
test_data  <- svm_data[test_idx, ]
# 建立 SVM 模型（RBF kernel, 預設 C=1, gamma=1/n_features）
svm_model_default <- svm(WL_num ~ ., data = train_data, kernel = "radial", scale = TRUE)
# 預測
pred <- predict(svm_model_default, test_data)
# 計算準確率
acc <- mean(pred == test_data$WL_num)
accuracy_list_default <- c(accuracy_list_default, acc)
}
cat("【基礎 SVM】5 折交叉驗證平均準確率:", round(mean(accuracy_list_default),4), "\n")
# --- Step 3. 超參數調優（Grid Search）以尋找最佳模型 (使用 tune 函數繞過 tune.svm 潛在衝突) ---
cat("\n--- 開始 SVM 超參數調優 (Grid Search) ---\n")
set.seed(456)
# A. 定義要搜索的參數範圍
tune_parameters <- list(
cost = 10^(-1:3), # 0.1, 1, 10, 100, 1000
gamma = 10^(-3:1) # 0.001, 0.01, 0.1, 1, 10
)
# B. 定義交叉驗證控制參數
my_tune_control <- tune.control(sampling = "cross", cross = 5)
# C. 運行網格搜索：直接使用 tune() 函數
# 明確指定 method = "svm"
svm_tuned <- tune(
method = "svm",
train.x = WL_num ~ .,
data = svm_data,
# 以下是 svm 模型的固定參數
kernel = "radial",
scale = TRUE,
# 以下是 tune 的參數
ranges = tune_parameters,
tunecontrol = my_tune_control
)
# C. 運行網格搜索：直接使用 tune() 函數
# 明確指定 method = "svm"
svm_tuned <- tune(
METHOD = "svm",
train.x = WL_num ~ .,
data = svm_data,
# 以下是 svm 模型的固定參數
kernel = "radial",
scale = TRUE,
# 以下是 tune 的參數
ranges = tune_parameters,
tunecontrol = my_tune_control
)
num_vars <- names(GameLogs2)[sapply(GameLogs2, is.numeric)]
num_vars <- setdiff(num_vars, c("WL_num", "GameNo", "error"))
svm_data <- GameLogs2 %>%
select(all_of(c("WL_num", num_vars)))
svm_data$WL_num <- as.factor(svm_data$WL_num) # SVM 分類
# --- Step 2. K 折交叉驗證 (使用預設參數) ---
set.seed(123)
K <- 5
n <- nrow(svm_data)
folds <- sample(rep(1:K, length.out = n))
accuracy_list_default <- c()
for(k in 1:K){
train_idx <- which(folds != k)
test_idx  <- which(folds == k)
train_data <- svm_data[train_idx, ]
test_data  <- svm_data[test_idx, ]
# 建立 SVM 模型（RBF kernel, 預設 C=1, gamma=1/n_features）
svm_model_default <- svm(WL_num ~ ., data = train_data, kernel = "radial", scale = TRUE)
# 預測
pred <- predict(svm_model_default, test_data)
# 計算準確率
acc <- mean(pred == test_data$WL_num)
accuracy_list_default <- c(accuracy_list_default, acc)
}
cat("【基礎 SVM】5 折交叉驗證平均準確率:", round(mean(accuracy_list_default),4), "\n")
# --- Step 3. 超參數調優（Grid Search）以尋找最佳模型 (使用 tune 函數繞過 tune.svm 潛在衝突) ---
cat("\n--- 開始 SVM 超參數調優 (Grid Search) ---\n")
set.seed(456)
# A. 定義要搜索的參數範圍
tune_parameters <- list(
cost = 10^(-1:3), # 0.1, 1, 10, 100, 1000
gamma = 10^(-3:1) # 0.001, 0.01, 0.1, 1, 10
)
# B. 定義交叉驗證控制參數
my_tune_control <- tune.control(sampling = "cross", cross = 5)
# C. 運行網格搜索：直接使用 tune() 函數
# 明確指定 method = "svm"
svm_tuned <- tune(
METHOD = "svm",
train.x = WL_num ~ .,
data = svm_data,
# 以下是 svm 模型的固定參數
kernel = "radial",
scale = TRUE,
# 以下是 tune 的參數
ranges = tune_parameters,
tunecontrol = my_tune_control
)
best_C <- svm_tuned$best.parameters$cost
best_gamma <- svm_tuned$best.parameters$gamma
cat("最佳參數組合 C =", best_C, ", gamma =", best_gamma, "\n")
cat("5 折交叉驗證最低錯誤率 (1 - Accuracy):", round(svm_tuned$best.performance, 4), "\n")
cat("預估準確率 (Accuracy):", round(1 - svm_tuned$best.performance, 4), "\n")
cat("--- 超參數調優完成 ---\n")
# --- Step 4, 5, 6 照常運行 ---
# ...
# --- Step 4. 用最佳參數建立最終模型 ---
# 使用最佳 C 和 gamma，並開啟 probability = TRUE 進行機率預測
svm_final_optimized <- svm(
WL_num ~ .,
data = svm_data,
kernel = "radial",
scale = TRUE,
probability = TRUE,
cost = best_C,
gamma = best_gamma
)
# --- Step 5. 預測逐場勝負 (使用優化後的模型) ---
svm_pred_prob <- predict(svm_final_optimized, svm_data, probability = TRUE)
# 提取 WL_num=1 (勝利) 的機率
GameLogs2$svm_pred_prob <- attr(svm_pred_prob, "probabilities")[, "1"]
# 計算預測誤差 (實際勝負 - 預測機率)
GameLogs2$svm_error <- abs(GameLogs2$svm_pred_prob - GameLogs2$WL_num)
# --- Step 6. 計算每隊平均預測勝率 ---
team_summary_svm_optimized <- GameLogs2 %>%
group_by(TEAM_ABBREVIATION) %>%
summarise(
avg_pred_prob = mean(svm_pred_prob, na.rm = TRUE),
actual_win_rate = mean(WL_num, na.rm = TRUE),
.groups = "drop"
)
# --- Step 7. 畫散點圖 ---
ggplot(team_summary_svm_optimized, aes(x = actual_win_rate, y = avg_pred_prob, label = TEAM_ABBREVIATION)) +
geom_point(color = "darkblue", size = 3) +
geom_text(vjust = -0.5, size = 3) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(
title = paste0("SVM 模型（超參數調優）：球隊預測勝率 vs 真實勝率"),
subtitle = paste0("最佳參數：C=", best_C, ", γ=", best_gamma),
x = "真實勝率",
y = "預測勝率"
) +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5, size = 9, color = "darkgreen"))
set.seed(999)
idx <- sample(1:nrow(svm_data), 0.8 * nrow(svm_data))
train <- svm_data[idx, ]
test  <- svm_data[-idx, ]
svm_overfit <- svm(
WL_num ~ ., data = train,
kernel = "radial",
cost = best_C, gamma = best_gamma
)
pred_test <- predict(svm_overfit, test)
mean(pred_test == test$WL_num)
num_vars <- names(GameLogs2)[sapply(GameLogs2, is.numeric)]
num_vars <- setdiff(num_vars, c("WL_num", "GameNo", "error"))
svm_data <- GameLogs2 %>%
select(all_of(c("WL_num", num_vars)))
svm_data$WL_num <- as.factor(svm_data$WL_num) # SVM 分類
# --- Step 2. K 折交叉驗證 (使用預設參數) ---
set.seed(123)
K <- 5
n <- nrow(svm_data)
folds <- sample(rep(1:K, length.out = n))
accuracy_list_default <- c()
for(k in 1:K){
train_idx <- which(folds != k)
test_idx  <- which(folds == k)
train_data <- svm_data[train_idx, ]
test_data  <- svm_data[test_idx, ]
# 建立 SVM 模型（RBF kernel, 預設 C=1, gamma=1/n_features）
svm_model_default <- svm(WL_num ~ ., data = train_data, kernel = "radial", scale = TRUE)
# 預測
pred <- predict(svm_model_default, test_data)
# 計算準確率
acc <- mean(pred == test_data$WL_num)
accuracy_list_default <- c(accuracy_list_default, acc)
}
cat("【基礎 SVM】5 折交叉驗證平均準確率:", round(mean(accuracy_list_default),4), "\n")
# --- Step 3. 超參數調優（Grid Search）以尋找最佳模型 (使用 tune 函數繞過 tune.svm 潛在衝突) ---
cat("\n--- 開始 SVM 超參數調優 (Grid Search) ---\n")
set.seed(456)
# A. 定義要搜索的參數範圍
tune_parameters <- list(
cost = c(0.1, 1, 3, 10, 30, 100),
gamma = c(0.0005, 0.001, 0.003, 0.01, 0.03)
)
# B. 定義交叉驗證控制參數
my_tune_control <- tune.control(sampling = "cross", cross = 5)
# C. 運行網格搜索：直接使用 tune() 函數
# 明確指定 method = "svm"
svm_tuned <- tune(
METHOD = "svm",
train.x = WL_num ~ .,
data = svm_data,
# 以下是 svm 模型的固定參數
kernel = "radial",
scale = TRUE,
# 以下是 tune 的參數
ranges = tune_parameters,
tunecontrol = my_tune_control
)
best_C <- svm_tuned$best.parameters$cost
best_gamma <- svm_tuned$best.parameters$gamma
cat("最佳參數組合 C =", best_C, ", gamma =", best_gamma, "\n")
cat("5 折交叉驗證最低錯誤率 (1 - Accuracy):", round(svm_tuned$best.performance, 4), "\n")
cat("預估準確率 (Accuracy):", round(1 - svm_tuned$best.performance, 4), "\n")
cat("--- 超參數調優完成 ---\n")
# --- Step 4, 5, 6 照常運行 ---
# ...
# --- Step 4. 用最佳參數建立最終模型 ---
# 使用最佳 C 和 gamma，並開啟 probability = TRUE 進行機率預測
svm_final_optimized <- svm(
WL_num ~ .,
data = svm_data,
kernel = "radial",
scale = TRUE,
probability = TRUE,
cost = best_C,
gamma = best_gamma
)
# --- Step 5. 預測逐場勝負 (使用優化後的模型) ---
svm_pred_prob <- predict(svm_final_optimized, svm_data, probability = TRUE)
# 提取 WL_num=1 (勝利) 的機率
GameLogs2$svm_pred_prob <- attr(svm_pred_prob, "probabilities")[, "1"]
# 計算預測誤差 (實際勝負 - 預測機率)
GameLogs2$svm_error <- abs(GameLogs2$svm_pred_prob - GameLogs2$WL_num)
# --- Step 6. 計算每隊平均預測勝率 ---
team_summary_svm_optimized <- GameLogs2 %>%
group_by(TEAM_ABBREVIATION) %>%
summarise(
avg_pred_prob = mean(svm_pred_prob, na.rm = TRUE),
actual_win_rate = mean(WL_num, na.rm = TRUE),
.groups = "drop"
)
# --- Step 7. 畫散點圖 ---
ggplot(team_summary_svm_optimized, aes(x = actual_win_rate, y = avg_pred_prob, label = TEAM_ABBREVIATION)) +
geom_point(color = "darkblue", size = 3) +
geom_text(vjust = -0.5, size = 3) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(
title = paste0("SVM 模型（超參數調優）：球隊預測勝率 vs 真實勝率"),
subtitle = paste0("最佳參數：C=", best_C, ", γ=", best_gamma),
x = "真實勝率",
y = "預測勝率"
) +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5, size = 9, color = "darkgreen"))
num_vars <- names(GameLogs2)[sapply(GameLogs2, is.numeric)]
num_vars <- setdiff(num_vars, c("WL_num", "GameNo", "error"))
svm_data <- GameLogs2 %>%
select(all_of(c("WL_num", num_vars)))
svm_data$WL_num <- as.factor(svm_data$WL_num)
# --- Step 2. K 折交叉驗證 ---
set.seed(123)
K <- 5
n <- nrow(svm_data)
folds <- sample(rep(1:K, length.out = n))
accuracy_list <- c()
for(k in 1:K){
train_idx <- which(folds != k)
test_idx  <- which(folds == k)
train_data <- svm_data[train_idx, ]
test_data  <- svm_data[test_idx, ]
# 使用預設 SVM 模型（RBF kernel）
svm_model <- svm(WL_num ~ ., data = train_data, kernel = "radial", scale = TRUE)
pred <- predict(svm_model, test_data)
acc <- mean(pred == test_data$WL_num)
accuracy_list <- c(accuracy_list, acc)
}
cat("【基礎 SVM】5 折交叉驗證平均準確率:", round(mean(accuracy_list),4), "\n")
# --- Step 3. 建立最終模型（使用全部資料） ---
svm_final <- svm(
WL_num ~ .,
data = svm_data,
kernel = "radial",
scale = TRUE,
probability = TRUE
)
# 預測機率
svm_pred_prob <- predict(svm_final, svm_data, probability = TRUE)
GameLogs2$svm_pred_prob <- attr(svm_pred_prob, "probabilities")[, "1"]
# 預測誤差
GameLogs2$svm_error <- abs(GameLogs2$svm_pred_prob - GameLogs2$WL_num)
# --- Step 4. 計算每隊平均預測勝率 ---
team_summary_svm <- GameLogs2 %>%
group_by(TEAM_ABBREVIATION) %>%
summarise(
avg_pred_prob = mean(svm_pred_prob, na.rm = TRUE),
actual_win_rate = mean(WL_num, na.rm = TRUE),
.groups = "drop"
)
# --- Step 5. 視覺化：預測 vs 真實勝率 ---
ggplot(team_summary_svm, aes(x = actual_win_rate, y = avg_pred_prob, label = TEAM_ABBREVIATION)) +
geom_point(color = "darkblue", size = 3) +
geom_text(vjust = -0.5, size = 3) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(
title = "SVM 模型：球隊預測勝率 vs 真實勝率（無超參數調整）",
x = "真實勝率",
y = "預測勝率"
) +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5))
